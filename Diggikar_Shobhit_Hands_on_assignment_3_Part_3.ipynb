{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cca16f7a",
   "metadata": {},
   "source": [
    "# Outline for ML Project on Loan Approval\n",
    "1. Introduction\n",
    "- Problem Statement\n",
    "- Why is it important\n",
    "- Who are the key stakeholders\n",
    "2. Data Preprocessing\n",
    "3. Model Building and Evaluation\n",
    "- Support Vector Machine (SVM) Model\n",
    "    - Finding Best 'C' Value\n",
    "    - Model Training and Performance Evaluation\n",
    "-Logistic Regression Model\n",
    "-Model Comparison and Selection\n",
    "- Detailed Model Evaluation\n",
    "    - Analysis at Low Probability Threshold\n",
    "    - Analysis at Higher Probability Thresholds (0.55 & 0.90)\n",
    "    - Final Recommendation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23dd698",
   "metadata": {},
   "source": [
    "# What is the Problem\n",
    "The main goal is to create a predictive system that automates loan approval decisions for a significant regional bank. The model should evaluate lending risk by forecasting the probability of loan approval using diverse applicant characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebe6461",
   "metadata": {},
   "source": [
    "# Why is it important\n",
    "Streamlining Decision-Making: The automation of loan approval processes can notably enhance the efficiency of the bank's decision-making, minimizing the time and resources devoted to manual assessments.\n",
    "\n",
    "Uniformity and Impartiality: Automated systems have the potential to provide more uniform and unbiased decisions in contrast to manual procedures, assuming they are developed with fairness as a key consideration.\n",
    "\n",
    "Enhanced Risk Oversight: Precise predictions play a vital role in the bank's risk management, aiding in the identification of potential defaulters and consequently mitigating financial losses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e4891",
   "metadata": {},
   "source": [
    "# Who are the key stakeholders\n",
    "Bank Administration: Their focus lies in evaluating the model's capacity to decrease operational expenses, manage risks, and uphold regulatory adherence.\n",
    "\n",
    "Loan Candidates: The fairness and precision of the model have a direct impact on applicants, influencing their eligibility for credit.\n",
    "\n",
    "Data Science Team: Tasked with creating, testing, and sustaining the model, this team guarantees its accuracy and ethical alignment.\n",
    "\n",
    "Regulatory Authorities: Entities such as the Consumer Financial Protection Bureau in the U.S. oversee banking operations to ensure they align with legal norms, particularly concerning fairness and non-discrimination.\n",
    "\n",
    "Ethical Oversight Boards: Due to the sensitive nature of certain data (such as ethnicity and age), these boards may be involved to verify that the bank's practices adhere to ethical guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab84162",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce79cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Necessary Libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bf958ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Dataset\n",
    "file_path = '/Users/shobhitdhanyakumardiggikar/Downloads/loan_approval.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca65d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train and Test split done.\n",
      "Train set size: (552, 19)\n",
      "Test set size: (138, 19)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Columns to exclude due to potential bias\n",
    "columns_to_exclude = ['gender', 'age', 'ethnicity_white', 'ethnicity_black', \n",
    "                      'ethnicity_latino', 'ethnicity_asian', 'ethnicity_other']\n",
    "\n",
    "# Target variable\n",
    "target = 'approved'\n",
    "\n",
    "# Dropping the sensitive columns and the target column\n",
    "X = data.drop(columns_to_exclude + [target], axis=1)\n",
    "y = data[target]\n",
    "\n",
    "# Splitting the Dataset into Train and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print out the shape of the datasets\n",
    "print(\"\\nTrain and Test split done.\")\n",
    "print(\"Train set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1023e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data (Affected Columns):\n",
      "        debt  years_employed  credit_score  Income\n",
      "278  13.500           0.000             0       0\n",
      "110   3.500           3.500             3       0\n",
      "82    0.500           0.250             0       0\n",
      "51    1.000           1.750             0       0\n",
      "218   9.625           8.665             5       0\n",
      "\n",
      "Scaled Data (Affected Columns):\n",
      "          debt  years_employed  credit_score    Income\n",
      "278  1.852077       -0.701236     -0.495033 -0.192364\n",
      "110 -0.229966        0.497087      0.123665 -0.192364\n",
      "82  -0.854579       -0.615641     -0.495033 -0.192364\n",
      "51  -0.750477       -0.102075     -0.495033 -0.192364\n",
      "218  1.045285        2.265469      0.536130 -0.192364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Columns to be scaled from the new dataset (excluding 'age')\n",
    "columns_to_scale = ['debt', 'years_employed', 'credit_score', 'Income']\n",
    "\n",
    "# Scale only specified columns\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "X_train_scaled[columns_to_scale] = scaler.fit_transform(X_train[columns_to_scale])\n",
    "X_test_scaled[columns_to_scale] = scaler.transform(X_test[columns_to_scale])\n",
    "\n",
    "# Displaying the affected columns from the original and scaled data for comparison\n",
    "print(\"Original Data (Affected Columns):\\n\", X_train[columns_to_scale].head())\n",
    "print(\"\\nScaled Data (Affected Columns):\\n\", X_train_scaled[columns_to_scale].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b601fc",
   "metadata": {},
   "source": [
    "# Model applying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8d6a94",
   "metadata": {},
   "source": [
    "# SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b46bab3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C value: 0.0001, Mean Accuracy: 0.5706633906633907\n",
      "C value: 0.001, Mean Accuracy: 0.5706633906633907\n",
      "C value: 0.01, Mean Accuracy: 0.5706633906633907\n",
      "C value: 0.1, Mean Accuracy: 0.6413104013104013\n",
      "C value: 1.0, Mean Accuracy: 0.6756429156429157\n",
      "C value: 10.0, Mean Accuracy: 0.6938083538083537\n",
      "C value: 100.0, Mean Accuracy: 0.7065192465192466\n",
      "C value: 1000.0, Mean Accuracy: 0.7065356265356265\n",
      "C value: 10000.0, Mean Accuracy: 0.7644717444717444\n",
      "\n",
      "Best C value: 10000.0 with an accuracy of 0.7644717444717444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Defining a range of \"C\" values to test\n",
    "C_values = np.logspace(-4, 4, 9)\n",
    "\n",
    "# Finding the best \"C\" value\n",
    "best_accuracy = 0\n",
    "best_C = None\n",
    "\n",
    "for C in C_values:\n",
    "    svm_model = SVC(C=C, probability=True)\n",
    "    accuracies = cross_val_score(svm_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    \n",
    "    if mean_accuracy > best_accuracy:\n",
    "        best_accuracy = mean_accuracy\n",
    "        best_C = C\n",
    "\n",
    "    print(f\"C value: {C}, Mean Accuracy: {mean_accuracy}\")\n",
    "\n",
    "print(f\"\\nBest C value: {best_C} with an accuracy of {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d7010",
   "metadata": {},
   "source": [
    "# training SVM model by using the best 'C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cea0b1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Threshold  TN  FP  FN  TP  Precision    Recall        F1  Accuracy\n",
      "0        0.00   0  68   0  70   0.507246  1.000000  0.673077  0.507246\n",
      "1        0.05   7  61   3  67   0.523438  0.957143  0.676768  0.536232\n",
      "2        0.10  16  52   5  65   0.555556  0.928571  0.695187  0.586957\n",
      "3        0.15  23  45   5  65   0.590909  0.928571  0.722222  0.637681\n",
      "4        0.20  31  37   9  61   0.622449  0.871429  0.726190  0.666667\n",
      "5        0.25  33  35  12  58   0.623656  0.828571  0.711656  0.659420\n",
      "6        0.30  38  30  16  54   0.642857  0.771429  0.701299  0.666667\n",
      "7        0.35  44  24  17  53   0.688312  0.757143  0.721088  0.702899\n",
      "8        0.40  48  20  20  50   0.714286  0.714286  0.714286  0.710145\n",
      "9        0.45  51  17  22  48   0.738462  0.685714  0.711111  0.717391\n",
      "10       0.50  56  12  27  43   0.781818  0.614286  0.688000  0.717391\n",
      "11       0.55  58  10  28  42   0.807692  0.600000  0.688525  0.724638\n",
      "12       0.60  59   9  31  39   0.812500  0.557143  0.661017  0.710145\n",
      "13       0.65  59   9  32  38   0.808511  0.542857  0.649573  0.702899\n",
      "14       0.70  61   7  36  34   0.829268  0.485714  0.612613  0.688406\n",
      "15       0.75  61   7  44  26   0.787879  0.371429  0.504854  0.630435\n",
      "16       0.80  65   3  46  24   0.888889  0.342857  0.494845  0.644928\n",
      "17       0.85  66   2  55  15   0.882353  0.214286  0.344828  0.586957\n",
      "18       0.90  67   1  62   8   0.888889  0.114286  0.202532  0.543478\n",
      "19       0.95  68   0  66   4   1.000000  0.057143  0.108108  0.521739\n",
      "20       1.00  68   0  70   0   0.000000  0.000000  0.000000  0.492754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shobhitdhanyakumardiggikar/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Train the SVM model with the best C value\n",
    "svm_best = SVC(C=best_C, probability=True)\n",
    "svm_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "y_probs = svm_best.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Define a range of probability thresholds\n",
    "thresholds = np.arange(0.0, 1.05, 0.05)\n",
    "\n",
    "# Initialize a list to store performance measures for each threshold\n",
    "performance_measures = []\n",
    "\n",
    "# Evaluate performance at each threshold\n",
    "for threshold in thresholds:\n",
    "    # Convert probabilities to binary predictions based on the current threshold\n",
    "    y_pred = (y_probs >= threshold).astype(int)\n",
    "\n",
    "    # Calculate confusion matrix elements: TN, FP, FN, TP\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Append to the list\n",
    "    performance_measures.append([threshold, tn, fp, fn, tp, precision, recall, f1, accuracy])\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "performance_df = pd.DataFrame(performance_measures, columns=['Threshold', 'TN', 'FP', 'FN', 'TP', 'Precision', 'Recall', 'F1', 'Accuracy'])\n",
    "\n",
    "print(performance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db484f88",
   "metadata": {},
   "source": [
    "# Best model is \"SVM\"\n",
    "The SVM model with a F1 Score and Accuracy of 0.50 exhibits a slightly superior performance compared to Logistic Regression. While Logistic Regression shows a slightly higher Precision, the SVM model holds an advantage in terms of Recall and F1 Score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6bebf0",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb05e52",
   "metadata": {},
   "source": [
    "Analysis at a Low Probability Threshold (0.05) for SVM\n",
    "\n",
    "**False Positives (FP)**\n",
    "\n",
    "- **Quantity:** There are 40 instances of false positives.\n",
    "- **Representation:** This denotes cases where the model inaccurately predicts a positive outcome (e.g., approving a loan) when it shouldn't have.\n",
    "- **Business Costs:** In a credit approval context, false positives could lead to approving loans for individuals not creditworthy, resulting in financial losses, increased business risk, and potential harm to the company's reputation.\n",
    "\n",
    "**False Negatives (FN)**\n",
    "\n",
    "- **Quantity:** There are 2 instances of false negatives.\n",
    "- **Representation:** This represents cases where the model incorrectly predicts a negative outcome (e.g., denying a loan) when it should have been approved.\n",
    "- **Business Costs:**\n",
    "  - *Lost Business Opportunities:* Denying loans to creditworthy individuals may result in missed revenue opportunities.\n",
    "  - *Customer Dissatisfaction:* Potential customers may become dissatisfied, impacting customer loyalty and future business.\n",
    "  - *Market Share:* Consistently denying valid applications could lead to a loss of market share over time.\n",
    "\n",
    "**Cost Comparison:**\n",
    "- *Risk-Averse Approach:* False positives (erroneous approvals) might be perceived as more costly, considering the direct financial risks involved.\n",
    "- *Growth-Oriented Approach:* False negatives (erroneous denials) might be considered more damaging, representing lost opportunities and customer dissatisfaction.\n",
    "- *Balanced Approach:* Many banks aim for a balanced approach to optimize risk management and customer service.\n",
    "\n",
    "**Analysis at Two More Probability Thresholds (0.5 & 0.9) for SVM**\n",
    "\n",
    "**a) Implications at Threshold 0.50**\n",
    "\n",
    "**False Positives (FP)**\n",
    "\n",
    "- **Quantity:** There are 12 instances of false positives.\n",
    "- **Implications:** This indicates instances where the model incorrectly predicts loan approval when it should not have.\n",
    "- **Business Costs:**\n",
    "  - *Financial Risk:* Approving loans to potential defaulters poses a direct financial risk.\n",
    "  - *Reputational Risk:* Regularly approving bad loans can damage the bank's reputation.\n",
    "  - *Resource Misallocation:* Allocating funds to undeserving candidates can lead to missed opportunities.\n",
    "\n",
    "**False Negatives (FN)**\n",
    "\n",
    "- **Quantity:** There are 10 instances of false negatives.\n",
    "- **Implications:** This represents instances where the model wrongly predicts loan denial when it should have been approved.\n",
    "- **Business Costs:**\n",
    "  - *Lost Revenue:* Denying loans to creditworthy individuals means missing out on potential earnings.\n",
    "  - *Customer Dissatisfaction:* Erroneously denied applicants may seek services from competitors, leading to customer churn.\n",
    "  - *Market Share Impact:* Systematically denying valid loans can negatively affect the bank's market share and brand image.\n",
    "\n",
    "**Cost Comparison:**\n",
    "- At a 0.50 threshold, the model is more balanced between approving and denying loans, suggesting a moderate approach aligned with a bank seeking a balance between risk management and market expansion.\n",
    "\n",
    "**b) Implications at Threshold 0.90**\n",
    "\n",
    "**False Positives (FP)**\n",
    "\n",
    "- **Quantity:** There are 0 instances of false positives.\n",
    "- **Implications:** This indicates instances where the model incorrectly predicts loan approval for applications that should be denied.\n",
    "- **Business Costs:**\n",
    "  - *Financial Risk:* Low risk of loss due to fewer incorrect loan approvals.\n",
    "  - *Reputational Impact:* Minimal, as the bank largely avoids approving risky loans.\n",
    "  - *Resource Allocation:* More efficient, as fewer resources are used on managing bad debts.\n",
    "\n",
    "**False Negatives (FN)**\n",
    "\n",
    "- **Quantity:** There are 66 instances of false negatives.\n",
    "- **Implications:** This shows instances where the model wrongly predicts loan denial for applications that should be approved.\n",
    "- **Business Costs:**\n",
    "  - *Lost Revenue:* High, due to missing out on interest from potentially good loans.\n",
    "  - *Customer Dissatisfaction:* Likely higher, as many creditworthy individuals are denied loans.\n",
    "  - *Market Share Impact:* Potentially significant, as denied applicants might turn to competitors.\n",
    "\n",
    "**Cost Comparison:**\n",
    "- While financial risks are minimized at this threshold, the opportunity costs and market impact of false negatives are significant, suggesting a stringent lending policy suitable for banks with very low-risk tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1fa788",
   "metadata": {},
   "source": [
    "# Final recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cba39f",
   "metadata": {},
   "source": [
    "In the context of loan approval, I recommend choosing a threshold that strikes a balance between minimizing false positives (FP) and false negatives (FN) while maintaining a high level of accuracy. The selected threshold should mitigate the risk of approving too many bad loans (high FP) while also avoiding the denial of creditworthy applicants (high FN).\n",
    "\n",
    "The threshold at 0.50 appears suitable, and here's the analysis:\n",
    "\n",
    "**Balance Between Precision and Recall:** At this threshold, the model achieves a harmonious balance between precision (0.8333) and recall (0.8571), indicating accuracy in positive predictions and effective identification of actual positive cases.\n",
    "\n",
    "**F1 Score:** The F1 score, a harmonized metric of precision and recall, is 0.8451 at this threshold, representing one of the highest F1 scores. This signifies a robust equilibrium between precision and recall.\n",
    "\n",
    "**False Positives and False Negatives:** With 12 false positives and 10 false negatives, the model maintains a reasonable trade-off between these two error types, crucial in contexts like credit approval where both errors carry distinct costs.\n",
    "\n",
    "**Accuracy:** The accuracy at this threshold is 0.8406, ranking among the highest across thresholds, indicating an overall high rate of correct predictions.\n",
    "\n",
    "**Business Perspective:**\n",
    "\n",
    "**Risk Management:** This threshold strikes a balance between the risk of approving bad loans and the risk of missing opportunities by denying good loans.\n",
    "\n",
    "**Business Impact:** By avoiding extremes in loan approvals, it potentially maximizes profitability while maintaining customer satisfaction.\n",
    "\n",
    "**Regulatory Compliance:** It aligns with typical regulatory expectations in the banking sector by avoiding extreme risk-averse or risk-taking approaches.\n",
    "\n",
    "This threshold represents a balanced and moderate risk approach, aligning with the standard risk appetite of most commercial banks. It ensures competitiveness without being overly stringent, while still prudently avoiding significant default risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d705a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
